---
title: "Optimization & Traces"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Optimization & Traces}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dpi = 150,
  dev = "png",
  out.width = "100%",
  fig.retina = 2
)
```

```{r, message=FALSE}
library(vistool)
library(plotly)
library(purrr)
set.seed(1)
```

This vignette covers the available optimizers, step size control, and how to visualize optimization traces.

## Optimizers

The optimizer class defines the optimization strategy and is initialized by taking an objective function, start value, and learning rate.
Available optimizers are:

- Gradient descent with `OptimizerGD`
- Momentum with `OptimizerMomentum`
- Nesterov's momentum with `OptimizerNAG`

Creating an optimizer is done by (let's use an x value that works well):

```{r}
obj = obj("TF_GoldsteinPriceLog")
opt = OptimizerGD$new(obj, x_start = c(0.22, 0.77), lr = 0.01)
```

With these values set, run `$optimize()` with the number of steps:

```{r}
opt$optimize(10L)
```

Calling `$optimize()` also writes into the archive of the optimizer and also calls `$eval_store()` of the objective.
Therefore, `$optimize()` writes into two archives:

```{r}
opt$archive
opt$objective$archive
```

## Visualize Optimization Traces

A layer of the `Visualizer` class is `$add_optimization_trace()` that gets the optimizer as argument and adds the optimization trace to the plot:

```{r, out.width='100%', out.height='700px'}
viz = as_visualizer(obj, type = "surface")
viz$add_optimization_trace(opt, name = "GD")
viz$plot()
```

## Step size control

`$optimize()` accepts a second argument `step_size_control` to scale the parameter update.
For GD with $x_{\text{new}} = x_{\text{old}} + lr * \Delta_f(x_{\text{old}})$, the update $u = lr * \Delta_f(x_{\text{old}})$ is multiplied by the return value of `step_size_control()`.
There are a few pre-implemented control functions like line search or various decaying methods:

- `step_size_control_line_search(lower, upper)`: Conduct a line search for $a$ in $x_{\text{new}} = x_{\text{old}} + a \cdot lr \cdot \Delta_f(x_{\text{old}})$.
- `step_size_control_decay_time(decay) `: Lower the updates by $(1 + decay * iteration)^{-1}$.
- `step_size_control_decay_exp(decay)`: Lower the updates by $exp(-decay * iteration)$.
- `step_size_control_decay_linear(iter_zero)`: Lower the updates until `iter_zero` is reached. Updates with `iter > iter_zero` are 0.
- `step_size_control_decay_steps(drop_rate, every_iter)`: Lower the updates `every_iter` by `drop_rate`.

Note that these functions return a function that contains a function with the required signature:

```{r}
step_size_control_decay_time()
```

Let's define multiple gradient descent optimizers and optimize 10 steps with a step size control:

```{r}
x0 = c(0.22, 0.77)
lr = 0.01

oo1 = OptimizerGD$new(obj, x_start = x0, lr = lr, id = "GD without LR Control", print_trace = FALSE)
oo2 = OptimizerGD$new(obj, x_start = x0, lr = lr, id = "GD with Line Search", print_trace = FALSE)
oo3 = OptimizerGD$new(obj, x_start = x0, lr = lr, id = "GD with Time Decay", print_trace = FALSE)
oo4 = OptimizerGD$new(obj, x_start = x0, lr = lr, id = "GD with Exp Decay", print_trace = FALSE)
oo5 = OptimizerGD$new(obj, x_start = x0, lr = lr, id = "GD with Linear Decay", print_trace = FALSE)
oo6 = OptimizerGD$new(obj, x_start = x0, lr = lr, id = "GD with Step Decay", print_trace = FALSE)

oo1$optimize(steps = 10)
oo2$optimize(steps = 10, step_size_control_line_search())
oo3$optimize(steps = 10, step_size_control_decay_time())
oo4$optimize(steps = 10, step_size_control_decay_exp())
oo5$optimize(steps = 10, step_size_control_decay_linear())
oo6$optimize(steps = 10, step_size_control_decay_steps())
```

For now we don't know how well it worked. Let's collect all archives with `merge_optim_archives()` and visualize the step sizes and function values with `patchwork` magic:

```{r}
arx = merge_optim_archives(oo1, oo2, oo3, oo4, oo5, oo6)

library(ggplot2)
library(patchwork)
gg1 = ggplot(arx, aes(x = iteration, y = step_size, color = optim_id))
gg2 = ggplot(arx, aes(x = iteration, y = fval_out, color = optim_id))

(gg1 + ggtitle("Step sizes") |
  gg1 + ylim(0, 1) + ggtitle("Step sizes (zoomed)") |
  gg2 + ggtitle("Objective")) +
  plot_layout(guides = "collect") &
  geom_line() &
  theme_minimal() &
  theme(legend.position = "bottom") &
  ggsci::scale_color_simpsons()
```

Visualizing the traces is done as before by adding optimization trace layer.
We can do this for all optimizers to add multiple traces to the plot:

```{r, out.width='100%', out.height='700px'}
viz = as_visualizer(obj, type = "surface")

viz$add_optimization_trace(oo1)
viz$add_optimization_trace(oo2)
viz$add_optimization_trace(oo3)
viz$add_optimization_trace(oo4)
viz$add_optimization_trace(oo5)
viz$add_optimization_trace(oo6)

viz$plot()
```

Practically, it should be no issue to also combine multiple control functions.
The important thing is to keep the signature of the function by allowing the function to get the arguments `x` (current value), `u` (current update), `obj` (`Objective` object), and `opt` (`Optimizer` object):

```{r}
myStepSizeControl = function(x, u, obj, opt) {
  sc1 = step_size_control_line_search(0, 10)
  sc2 = step_size_control_decay_time(0.1)
  return(sc1(x, u, obj, opt) * sc2(x, u, obj, opt))
}

my_oo = OptimizerGD$new(obj, x_start = x0, lr = lr, id = "GD without LR Control", print_trace = FALSE)
my_oo$optimize(100, myStepSizeControl)
tail(my_oo$archive)
```

## Customization options

Let's optimize a custom linear model objective (see the [Objective Functions vignette](objective.html)) using the three available optimizers.
```{r}
# Define the linear model loss function as SSE:
l2norm = function(x) sqrt(sum(crossprod(x)))

mylm = function(x, Xmat, y) {
  l2norm(y - Xmat %*% x)
}
# Use the iris dataset with response `Sepal.Width` and feature `Petal.Width`:
Xmat = model.matrix(~Petal.Width, data = iris)
y = iris$Sepal.Width

# Create a new object:
obj_lm = Objective$new(id = "iris LM", fun = mylm, xdim = 2, Xmat = Xmat, y = y, minimize = TRUE)

oo1 = OptimizerGD$new(obj_lm, x_start = c(0, -0.05), lr = 0.001, print_trace = FALSE)
oo2 = OptimizerMomentum$new(obj_lm, x_start = c(-0.05, 0), lr = 0.001, print_trace = FALSE)
oo3 = OptimizerNAG$new(obj_lm, x_start = c(0, 0), lr = 0.001, print_trace = FALSE)

oo1$optimize(steps = 100)
oo2$optimize(steps = 100)
oo3$optimize(steps = 100)
```

Optimization traces support many customization options (see the [Customization Guide](customization_guide.html)):

```{r, out.width='100%', out.height='700px'}
viz = as_visualizer(obj_lm, x1_limits = c(-0.5, 5), x2_limits = c(-3.2, 2.8), type = "surface")


viz$add_optimization_trace(oo1, add_marker_at = round(seq(1, 100, len = 10L)))
viz$add_optimization_trace(oo2, line_color = "yellow", add_marker_at = c(1, 50, 90), marker_shape = c("square", "diamond", "cross"))
viz$add_optimization_trace(oo3, line_color = "red", line_width = 20, line_type = "dashed")

viz$plot()
```

We can also use the alternative `ggplot2` backend:

```{r, out.width='100%', out.height='700px'}
viz_2d = as_visualizer(obj_lm, x1_limits = c(-0.5, 5), x2_limits = c(-3.2, 2.8))

viz_2d$add_optimization_trace(oo1,
  name = "Gradient Descent"
)

viz_2d$add_optimization_trace(oo2,
  line_type = "dashed",
  name = "Momentum"
)

viz_2d$add_optimization_trace(oo3,
  line_type = "dotted",
  name = "Nesterov AG"
)

viz_2d$plot()
```

## See also

- [Loss Functions](loss_functions.html)
- [Model Predictions](model.html)
- [Objective Functions](objective.html)
- [Customization Guide](customization_guide.html)
- [Advanced Visualization](advanced_visualization.html)
